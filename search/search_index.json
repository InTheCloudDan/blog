{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>I'm a Senior Ecosystem Engineer that works at LaunchDarkly.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"posts/","title":"Blog","text":""},{"location":"posts/2024/05/22/modifying-llm-finetune-example/","title":"Modifying llm-finetune Example","text":"<p>As part of the Mastering LLMs Conference that I'm attending, I fine-tuned my first model! Using credits provided by Modal and following their LLM Fine Tuning example guide, I was able to get it running.</p> <p>The next step I wanted to take was to modify the course example, available at this GitHub repo, to run in Modal.</p> <p>tl;dr after a bit of trial and error, it was easy to get running!</p>"},{"location":"posts/2024/05/22/modifying-llm-finetune-example/#weights-and-biases","title":"Weights and Biases","text":"<p>In the existing config, there was a section for <code>wandb</code> setup. I signed up for an account and modified my local run to be:</p> <pre><code>ALLOW_WANDB=true modal run --detach src.train\n</code></pre> <p>During that run, I received an error:</p> <pre><code>wandb: ERROR It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup. (Error 403: Forbidden)\n</code></pre> <p>The fix was to remove the <code>wandb_entity</code> key and, from the Modal docs, add <code>wandb_watch: gradients</code>.</p> <p></p>"},{"location":"posts/2024/05/22/modifying-llm-finetune-example/#diff-for-ftcourse","title":"Diff for ftcourse","text":"<p>I commented out the <code>dataset_prepared_path</code> as I wasn't sure if it was going to be needed. Additionally, I did not want to upload the model to HuggingFace, so I removed that line.</p> <pre><code>diff --git a/hc.yml b/hc.yml\nindex a57b51f..d8e9c73 100644\n--- a/hc.yml\n+++ b/hc.yml\n@@ -12,13 +12,12 @@ data_seed: 49\n seed: 49\n\n datasets:\n-  - path: sample_data/alpaca_synth_queries.jsonl\n+  - path: data.jsonl\n     type: sharegpt\n     conversation: alpaca\n-dataset_prepared_path: last_run_prepared\n+#dataset_prepared_path: last_run_prepared\n val_set_size: 0.1\n output_dir: ./qlora-alpaca-out\n-hub_model_id: hamel/hc-mistral-alpaca\n\n adapter: qlora\n lora_model_dir:\n@@ -41,8 +40,8 @@ lora_target_modules:\n   - k_proj\n   - o_proj\n\n-wandb_project: hc-axolotl-mistral\n-wandb_entity: hamelsmu\n+wandb_project: hc-axolotl-test\n+wandb_watch: gradients\n\n gradient_accumulation_steps: 4\n micro_batch_size: 16\n</code></pre> <p>The following command was run from the <code>llm-finetuning</code> directory:</p> <pre><code>ALLOW_WANDB=true modal run --detach src.train --config=/path/to/ftcourse/hc.yml --data=path/to/ftcourse/sample_data/alpaca_synth_queries.jsonl\n</code></pre>"},{"location":"posts/2024/05/22/modifying-llm-finetune-example/#conclusion","title":"Conclusion","text":"<p>I was able to complete the runs and use the inference API to interact with the model.</p> <p>The model's response was not exactly what I was hoping for, but I learned a lot!</p> <p>\ud83d\udc64: Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produces a Honeycomb query.</p> <p>Instruction: NLQ: 'Exception count by exception and caller'</p> <p>Columns: ['error', 'exception.message', 'exception.type', 'exception.stacktrace', 'SampleRate', 'name', 'db.user', 'type', 'duration_ms', 'db.name', 'service.name', 'http.method', 'db.system', 'status_code', 'db.operation', 'library.name', 'process.pid', 'net.transport', 'messaging.system', 'rpc.system', 'http.target', 'db.statement', 'library.version', 'status_message', 'parent_name', 'aws.region', 'process.command', 'rpc.method', 'span.kind', 'serializer.name', 'net.peer.name', 'rpc.service', 'http.scheme', 'process.runtime.name', 'serializer.format', 'serializer.renderer', 'net.peer.port', 'process.runtime.version', 'http.status_code', 'telemetry.sdk.language', 'trace.parent_id', 'process.runtime.description', 'span.num_events', 'messaging.destination', 'net.peer.ip', 'trace.trace_id', 'telemetry.instrumentation_library', 'trace.span_id', 'span.num_links', 'meta.signal_type', 'http.route']</p> <p>Response: \ud83e\udd16:  <pre><code>from honeycomb import Client, Query\n\nclient = Client(api_key='YOUR_API_KEY')\nquery = Query('''\n    from error\n    group by error\n    count() as count\n''')\nresponse = client.run_query(query)\nprint(response.json())\n</code></pre></p>"},{"location":"posts/archive/2024/","title":"2024","text":""},{"location":"posts/category/machine-learning/","title":"Machine Learning","text":""}]}